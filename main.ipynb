{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://github.com/facebookresearch/dino/zipball/main\" to /srv/home/zxu444/.cache/torch/hub/main.zip\n",
      "Downloading: \"https://dl.fbaipublicfiles.com/dino/dino_deitsmall16_pretrain/dino_deitsmall16_pretrain.pth\" to /srv/home/zxu444/.cache/torch/hub/checkpoints/dino_deitsmall16_pretrain.pth\n",
      "100%|██████████| 82.7M/82.7M [00:01<00:00, 61.3MB/s]\n"
     ]
    }
   ],
   "source": [
    "vits16 = torch.hub.load('facebookresearch/dino:main', 'dino_vits16')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VisionTransformer(\n",
       "  (patch_embed): PatchEmbed(\n",
       "    (proj): Conv2d(3, 384, kernel_size=(16, 16), stride=(16, 16))\n",
       "  )\n",
       "  (pos_drop): Dropout(p=0.0, inplace=False)\n",
       "  (blocks): ModuleList(\n",
       "    (0-11): 12 x Block(\n",
       "      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "  (head): Identity()\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vits16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import encoders, classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'convnet4': <function models.encoders.convnet4.convnet4()>,\n",
       " 'wide-convnet4': <function models.encoders.convnet4.wide_convnet4()>,\n",
       " 'resnet12': <function models.encoders.resnet12.resnet12()>,\n",
       " 'wide-resnet12': <function models.encoders.resnet12.wide_resnet12()>,\n",
       " 'resnet18': <function models.encoders.resnet18.resnet18()>,\n",
       " 'wide-resnet18': <function models.encoders.resnet18.wide_resnet18()>,\n",
       " 'OneLayerNN': models.encoders.wrapper.OneLayerNN,\n",
       " 'TwoLayersNN': models.encoders.wrapper.TwoLayersNN,\n",
       " 'twoLayersResNet': <function models.encoders.wrapper.twoLayersResNet(in_dim)>,\n",
       " 'wrapper': models.encoders.wrapper.Wrapper,\n",
       " 'clip_ViT-B32': models.encoders.ViT.clip_ViTB32,\n",
       " 'dinov2_vits14': <function models.encoders.ViT.dinov2_vits14()>,\n",
       " 'dinov2_vitb14': <function models.encoders.ViT.dinov2_vitb14()>,\n",
       " 'dinov2_vitl14': <function models.encoders.ViT.dinov2_vitl14()>,\n",
       " 'dinov2_vitg14': <function models.encoders.ViT.dinov2_vitg14()>,\n",
       " 'dino_vitb16': <function models.encoders.ViT.dino_vitb16()>,\n",
       " 'torchvision_vit_b_32': models.encoders.ViT.torchvision_vit_b_32,\n",
       " 'mocov3_vit': models.encoders.ViT.mocov3_vit,\n",
       " 'clip_RN50': models.encoders.pretrained_resnet.clip_RN50,\n",
       " 'ResNet50_mocov2': models.encoders.pretrained_resnet.ResNet50_mocov2,\n",
       " 'torchvision_RN50': models.encoders.pretrained_resnet.torchvision_RN50,\n",
       " 'mocov3_RN50': models.encoders.pretrained_resnet.mocov3_RN50}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoders.encoders.models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "convnet4\n",
      "<function convnet4 at 0x7fbc48322280>\n",
      "zhuyan: \n",
      "wide-convnet4\n",
      "<function wide_convnet4 at 0x7fbc48322310>\n",
      "zhuyan: \n",
      "resnet12\n",
      "<function resnet12 at 0x7fbc48322940>\n",
      "zhuyan: \n",
      "wide-resnet12\n",
      "<function wide_resnet12 at 0x7fbc483229d0>\n",
      "zhuyan: \n",
      "resnet18\n",
      "<function resnet18 at 0x7fbc482a8040>\n",
      "zhuyan: \n",
      "wide-resnet18\n",
      "<function wide_resnet18 at 0x7fbc482a80d0>\n",
      "zhuyan: \n",
      "OneLayerNN\n",
      "<class 'models.encoders.wrapper.OneLayerNN'>\n",
      "zhuyan: \n",
      "TwoLayersNN\n",
      "<class 'models.encoders.wrapper.TwoLayersNN'>\n",
      "zhuyan: \n",
      "twoLayersResNet\n",
      "<function twoLayersResNet at 0x7fbc482a8700>\n",
      "zhuyan: \n",
      "wrapper\n",
      "<class 'models.encoders.wrapper.Wrapper'>\n",
      "zhuyan: \n",
      "clip_ViT-B32\n",
      "<class 'models.encoders.ViT.clip_ViTB32'>\n",
      "zhuyan: \n",
      "dinov2_vits14\n",
      "<function dinov2_vits14 at 0x7fbc4369b0d0>\n",
      "zhuyan: \n",
      "dinov2_vitb14\n",
      "<function dinov2_vitb14 at 0x7fbc4369b160>\n",
      "zhuyan: \n",
      "dinov2_vitl14\n",
      "<function dinov2_vitl14 at 0x7fbc4369b1f0>\n",
      "zhuyan: \n",
      "dinov2_vitg14\n",
      "<function dinov2_vitg14 at 0x7fbc4369b280>\n",
      "zhuyan: \n",
      "dino_vitb16\n",
      "<function dino_vitb16 at 0x7fbc4369b4c0>\n",
      "zhuyan: \n",
      "torchvision_vit_b_32\n",
      "<class 'models.encoders.ViT.torchvision_vit_b_32'>\n",
      "zhuyan: \n",
      "mocov3_vit\n",
      "<class 'models.encoders.ViT.mocov3_vit'>\n",
      "zhuyan: \n",
      "clip_RN50\n",
      "<class 'models.encoders.pretrained_resnet.clip_RN50'>\n",
      "zhuyan: \n",
      "ResNet50_mocov2\n",
      "<class 'models.encoders.pretrained_resnet.ResNet50_mocov2'>\n",
      "zhuyan: \n",
      "torchvision_RN50\n",
      "<class 'models.encoders.pretrained_resnet.torchvision_RN50'>\n",
      "zhuyan: \n",
      "mocov3_RN50\n",
      "<class 'models.encoders.pretrained_resnet.mocov3_RN50'>\n",
      "zhuyan: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /srv/home/zxu444/.cache/torch/hub/facebookresearch_dino_main\n",
      "Downloading: \"https://dl.fbaipublicfiles.com/dino/dino_vitbase16_pretrain/dino_vitbase16_pretrain.pth\" to /srv/home/zxu444/.cache/torch/hub/checkpoints/dino_vitbase16_pretrain.pth\n",
      "100%|██████████| 327M/327M [00:05<00:00, 59.5MB/s] \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "dino_vit(\n",
       "  (model): VisionTransformer(\n",
       "    (patch_embed): PatchEmbed(\n",
       "      (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
       "    )\n",
       "    (pos_drop): Dropout(p=0.0, inplace=False)\n",
       "    (blocks): ModuleList(\n",
       "      (0-11): 12 x Block(\n",
       "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "    (head): Identity()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoders.make('dino_vitb16')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ood",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
